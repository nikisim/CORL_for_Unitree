{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import loco_mujoco\n",
    "from loco_mujoco import LocoEnv\n",
    "\n",
    "\n",
    "env = gym.make(\"LocoMujoco\", env_name=\"UnitreeA1.simple\")\n",
    "mdp = LocoEnv.make(\"UnitreeA1.simple.perfect\")\n",
    "\n",
    "# env = gym.make('HalfCheetah-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "success = [1.0,1.0,0.0]\n",
    "\n",
    "print(sum(success)/len(success))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/FetchPush'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENV_NAME = 'FetchPush'\n",
    "\n",
    "ste = f\"data/{ENV_NAME}\"\n",
    "ste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_dataset(dataset):\n",
    "\n",
    "    dataset['observations'] = dataset.pop('states')\n",
    "    dataset['actions'] = dataset.pop('actions')\n",
    "    dataset['next_observations'] = dataset.pop('next_states')\n",
    "    dataset['rewards'] = dataset.pop('rewards')\n",
    "    dataset['terminals'] = dataset.pop('last')\n",
    "\n",
    "    del dataset['absorbing']\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = mdp.create_dataset()\n",
    "dataset = reformat_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 37)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['observations'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87187545, 0.84193043, 0.7776974 , ..., 0.88017822, 0.86232611,\n",
       "       0.82762313])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['rewards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.75222744e-01, -7.73589914e-03, -8.11422792e-03,  9.87607437e-04,\n",
       "         1.83854901e-01,  9.15128876e-01, -2.26819727e+00, -6.37933476e-02,\n",
       "         9.34214899e-01, -1.91677393e+00, -4.05785975e-02,  9.36312879e-01,\n",
       "        -1.87045267e+00,  1.60763861e-01,  9.16133264e-01, -2.15280149e+00,\n",
       "         2.44099928e-01,  3.62274562e-02,  1.94768716e-02,  2.03384910e-01,\n",
       "         8.99440417e-02, -7.28740115e-03, -8.17263219e-01, -1.93174492e+00,\n",
       "         2.00346657e+00, -3.39504673e-01,  9.03097584e-01, -2.70741528e-02,\n",
       "        -3.43999288e-01,  7.30498063e-01,  2.36305918e-01, -5.87035927e-01,\n",
       "        -2.58242325e+00,  1.63759636e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         2.36557250e-01]),\n",
       " {})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikisim/.local/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     20\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(action_dim)\n\u001b[0;32m---> 21\u001b[0m nstate, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# HERE is your favorite RL algorithm\u001b[39;00m\n\u001b[1;32m     25\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Mag_diplom/loco-mujoco/loco_mujoco/environments/gymnasium.py:66\u001b[0m, in \u001b[0;36mGymnasiumWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    Run one timestep of the environment's dynamics.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     obs, reward, absorbing, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obs, reward, absorbing, \u001b[38;5;28;01mFalse\u001b[39;00m, info\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mushroom_rl/environments/mujoco.py:155\u001b[0m, in \u001b[0;36mMuJoCo.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulation_pre_step()\n\u001b[1;32m    153\u001b[0m mujoco\u001b[38;5;241m.\u001b[39mmj_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_substeps)\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulation_post_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recompute_action_per_step:\n\u001b[1;32m    158\u001b[0m     cur_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_observation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_helper\u001b[38;5;241m.\u001b[39m_build_obs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data))\n",
      "File \u001b[0;32m~/Mag_diplom/loco-mujoco/loco_mujoco/environments/quadrupeds/unitreeA1.py:451\u001b[0m, in \u001b[0;36mUnitreeA1._simulation_post_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_simulation_post_step\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    Sets the correct rotation of the goal arrow and the calculates the\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03m    statistics of the ground forces if required. This function is\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m    called after each step.\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_goal_arrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_simulation_post_step()\n",
      "File \u001b[0;32m~/Mag_diplom/loco-mujoco/loco_mujoco/environments/quadrupeds/unitreeA1.py:579\u001b[0m, in \u001b[0;36mUnitreeA1._set_goal_arrow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39msite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdir_arrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mxmat \u001b[38;5;241m=\u001b[39m (rot_mat)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m9\u001b[39m,))\n\u001b[1;32m    578\u001b[0m \u001b[38;5;66;03m# calc position of the ball corresponding to the arrow\u001b[39;00m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39msite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdir_arrow_ball\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mxpos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdir_arrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mxpos \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m    580\u001b[0m                                          [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msin(desired_angle), \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mcos(desired_angle), \u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from loco_mujoco import LocoEnv\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "# create the environment and task together with the reward function\n",
    "env = gym.make(\"LocoMujoco\", env_name=\"UnitreeA1.simple.perfect\")\n",
    "# env = gym.wrappers.RecordVideo(env, f\"videos/unitreeA1\", episode_trigger = lambda x: x % 10 == 0)\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "env.reset()\n",
    "env.render()\n",
    "terminated = False\n",
    "i = 0\n",
    "\n",
    "while True:\n",
    "    if i == 1000 or terminated:\n",
    "        env.reset()\n",
    "        i = 0\n",
    "    action = np.random.randn(action_dim)\n",
    "    nstate, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # HERE is your favorite RL algorithm\n",
    "\n",
    "    env.render()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikisim/.local/lib/python3.10/site-packages/gym/envs/mujoco/mujoco_env.py:360: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"HalfCheetah-v4\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/home/nikisim/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# action = actor.act(state, device)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     10\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/spaces/box.py:212\u001b[0m, in \u001b[0;36mBox.sample\u001b[0;34m(self, mask)\u001b[0m\n\u001b[1;32m    204\u001b[0m sample[unbounded] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnp_random\u001b[38;5;241m.\u001b[39mnormal(size\u001b[38;5;241m=\u001b[39munbounded[unbounded]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    206\u001b[0m sample[low_bounded] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnp_random\u001b[38;5;241m.\u001b[39mexponential(size\u001b[38;5;241m=\u001b[39mlow_bounded[low_bounded]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow[low_bounded]\n\u001b[1;32m    209\u001b[0m )\n\u001b[1;32m    211\u001b[0m sample[upp_bounded] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnp_random\u001b[49m\u001b[38;5;241m.\u001b[39mexponential(size\u001b[38;5;241m=\u001b[39mupp_bounded[upp_bounded]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhigh[upp_bounded]\n\u001b[1;32m    214\u001b[0m )\n\u001b[1;32m    216\u001b[0m sample[bounded] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnp_random\u001b[38;5;241m.\u001b[39muniform(\n\u001b[1;32m    217\u001b[0m     low\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow[bounded], high\u001b[38;5;241m=\u001b[39mhigh[bounded], size\u001b[38;5;241m=\u001b[39mbounded[bounded]\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    218\u001b[0m )\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/spaces/space.py:72\u001b[0m, in \u001b[0;36mSpace.np_random\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnp_random\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mGenerator:\n\u001b[1;32m     74\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lazily seed the PRNG since this is expensive and only needed if sampling from this space.\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_np_random \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "n_episodes = 5\n",
    "for _ in range(n_episodes):\n",
    "    state, done = env.reset(), False\n",
    "    env.render()\n",
    "    episode_reward = 0.0\n",
    "    while not done:\n",
    "        # action = actor.act(state, device)\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        env.render()\n",
    "        episode_reward += reward\n",
    "    # episode_rewards.append(episode_reward)\n",
    "    env.close()\n",
    "# Each task is associated with a dataset\n",
    "# dataset contains observations, actions, rewards, terminals, and infos\n",
    "# dataset = env.get_dataset()\n",
    "# print(dataset['observations']) # An N x dim_observation Numpy array of observations\n",
    "\n",
    "# # Alternatively, use d4rl.qlearning_dataset which\n",
    "# # also adds next_observations.\n",
    "# dataset = d4rl.qlearning_dataset(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
